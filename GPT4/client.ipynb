{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is organized for creating a proper job for \n",
    "# fine-tuning a GPT model of their choice. For our goal\n",
    "# we focused on the GPT-4o model primarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving API Key for fine-tuned GPT-4o model\n",
    "load_dotenv()\n",
    "FINE_TUNED_TOXIC_DETECTION_API_KEY = os.getenv(\"FINE_TUNED_TOXIC_DETECTION_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating client with API key\n",
    "client = OpenAI(\n",
    "    api_key=FINE_TUNED_TOXIC_DETECTION_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-UFWHTku5sU3K6GzVemhsZt0H', bytes=3847655, created_at=1727986652, filename='validation_kaggle_toxic_classification_dataset.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing files on OpenAI dashboard for training and validation\n",
    "client.files.create(\n",
    "  file=open(\"../data/training_kaggle_toxic_classification_dataset.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"../data/validation_kaggle_toxic_classification_dataset.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[FileObject](data=[FileObject(id='file-FpwsaDITa0co3Z4sNvzknGGX', bytes=7686481, created_at=1727979219, filename='kaggle_toxic_classification_dataset.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-gYnCm7EvYy3qnTJhpMJ5sNs7', bytes=78420036, created_at=1727915595, filename='kaggle_toxic_classification_dataset.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-sw9JIkTReL2JanwwHKAhNuiG', bytes=48420688, created_at=1727913270, filename='training.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)], object='list', has_more=False, first_id='file-FpwsaDITa0co3Z4sNvzknGGX', last_id='file-sw9JIkTReL2JanwwHKAhNuiG')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files status\n",
    "client.files.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-JAIhvjA1yAen4Yt6Utyh3289', created_at=1727986789, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-S652ryxRoR0nJQpnMy7g1xcm', result_files=[], seed=1154032060, status='validating_files', trained_tokens=None, training_file='file-zX0Hx54u69Y73wTuxyDfTuLN', validation_file='file-UFWHTku5sU3K6GzVemhsZt0H', estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# executing fine-tuning job with training and validation files\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-zX0Hx54u69Y73wTuxyDfTuLN\",\n",
    "    validation_file=\"file-UFWHTku5sU3K6GzVemhsZt0H\",\n",
    "    model=\"gpt-4o-2024-08-06\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-Mymd6RM7VvSJwOVBBpqNwx5p', created_at=1727979309, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:personal::AELSRgXJ', finished_at=1727982438, hyperparameters=Hyperparameters(n_epochs=1, batch_size=10, learning_rate_multiplier=2), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-S652ryxRoR0nJQpnMy7g1xcm', result_files=['file-5gCTWySFk0MgmOZ2YjQvyEZJ'], seed=69129836, status='succeeded', trained_tokens=1277205, training_file='file-FpwsaDITa0co3Z4sNvzknGGX', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking job progress/status\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-Mymd6RM7VvSJwOVBBpqNwx5p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = ''\n",
    "# Read the evaluation prompt from the text file with utf-8 encoding\n",
    "with open(\"../data/text/detailed_instructions.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    evaluation_prompt += file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task: You are given a set of comments from social media. Your goal is to evaluate each comment and classify it as either toxic or non-toxic. A comment is considered toxic if it contains harmful, abusive, offensive, or harassing language. If a comment is toxic, label it with a 1. Otherwise, if it is neutral or polite and does not contain harmful language, label it with a 0 for non-toxic.\\n\\nDefinition of Toxicity:\\nToxic comments (1): These are comments that include language or content that is rude, disrespectful, hateful, harmful, threatening, or harassing. This includes but is not limited to:\\n\\nProfanity or slurs (racial, gender-based, homophobic, etc.)\\nDirect or implied threats of violence or harm\\nPersonal attacks, name-calling, or derogatory statements\\nEncouragement of harm or violence toward individuals or groups\\nLanguage that promotes discrimination, hatred, or exclusion\\nInsults or offensive jokes directed at individuals or groups based on their identity, beliefs, or characteristics\\nNon-toxic comments (0): These are comments that maintain respect, politeness, and do not include any language that could be considered harmful or abusive. Non-toxic comments might still express disagreement or critique but do so in a civil, respectful manner without any form of harassment or offense.\\n\\nGuidelines for Labeling:\\nContext Matters: Evaluate each comment based on its context and the overall sentiment it conveys. Some words may be considered toxic in specific contexts but non-toxic in others (e.g., sarcasm or jokes).\\n\\nIgnore Typos and Grammar: Do not penalize comments for poor grammar or spelling mistakes. Focus purely on the intent and toxicity of the content.\\n\\nAmbiguity: If the comment is ambiguous or unclear in its meaning but could reasonably be interpreted as harmful or offensive, lean toward labeling it as toxic (1).\\n\\nFalse Positives/Negatives: Avoid false positives by ensuring the comment genuinely reflects harmful intent before labeling it as toxic. Likewise, avoid false negatives by considering subtler forms of toxicity, such as passive-aggressive or coded language.\\n\\nMild Criticism: Not all negative language is toxic. Comments that are critical but do not contain harmful intent, offensive language, or personal attacks should be labeled as non-toxic (0).\\n\\nExamples for Reference:\\nComment: “You’re an idiot and should be banned from this platform.”\\n\\nLabel: 1 (Toxic)\\nReason: This is a direct personal attack with abusive language.\\nComment: “I disagree with you, but I think you make some valid points.”\\n\\nLabel: 0 (Non-toxic)\\nReason: Polite disagreement without any harmful language.\\nComment: “Go back to where you came from, we don’t want you here.”\\n\\nLabel: 1 (Toxic)\\nReason: This is a xenophobic comment encouraging exclusion.\\nComment: “Your argument doesn’t make any sense at all.”\\n\\nLabel: 0 (Non-toxic)\\nReason: Although critical, the comment remains non-abusive.\\nComment: “You’re a joke and your opinion doesn’t matter.”\\n\\nLabel: 1 (Toxic)\\nReason: This is dismissive, disrespectful, and derogatory.\\nComment: “You must be joking, right? LOL, this is hilarious.”\\n\\nLabel: 0 (Non-toxic)\\nReason: This is a playful response that does not cross into harmful language.\\nComment: “I hope something bad happens to you.”\\n\\nLabel: 1 (Toxic)\\nReason: This is a harmful statement and implies wishing harm on someone.\\nInstructions:\\nFor each comment presented to you, classify it based on the guidelines provided. Use the binary labels:\\n\\n1 for toxic comments\\n0 for non-toxic comments'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts fine-tuned model with custom text and evaluation prompt\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"ft:gpt-4o-2024-08-06:personal::AENT6Q3Y\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": evaluation_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Woah, that was uncalled for\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# response from fine-tuned model\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
