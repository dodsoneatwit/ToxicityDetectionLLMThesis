{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports/libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary pip installation commands include:\n",
    "- pip install os\n",
    "- pip install openai\n",
    "- pip install python-dotenv\n",
    "\n",
    "#### <b>Note</b>: have a <b>.env</b> file already created for accessing API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests accuracy of chosen model against unique prompt and data\n",
    "def accuracy_testing(data, evaluation_prompt, model):\n",
    "    # chooses columns of focus\n",
    "    test = pd.DataFrame(columns=['Text', 'Toxic'])\n",
    "    for index, row in data.iterrows():\n",
    "        text = row['Text']\n",
    "        completion = test.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": evaluation_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        test.loc[index] = [text, completion.choices[0].message.content]\n",
    "    # comparing results of model to dataset\n",
    "    compare = test['Toxic'] == data['Toxic']\n",
    "    accuracy = compare.values.sum() / compare.size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .env file values \n",
    "load_dotenv()\n",
    "\n",
    "# insert OpenAI API key in here from .env\n",
    "FINE_TUNED_TOXIC_DETECTION_API_KEY = os.getenv(\"FINE_TUNED_TOXIC_DETECTION_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate OpenAI client with API key\n",
    "client = OpenAI(\n",
    "    api_key=FINE_TUNED_TOXIC_DETECTION_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves data to test accuracy against model of choice\n",
    "df = pd.read_csv('LOCATION OF YOUR DATASET')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Optional</b> for retrieving custom prompt for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = ''\n",
    "# Read the evaluation prompt from the text file with utf-8 encoding\n",
    "with open(\"./data/text/general/summarized_instructions.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    evaluation_prompt += file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link to models for testing: <b>https://platform.openai.com/docs/models</b>\n",
    "\n",
    "#### Here are a few working models:\n",
    "- <b>o1-preview</b>\n",
    "- <b>o1-mini</b>\n",
    "- <b>gpt-4o-2024-08-06</b>\n",
    "- <b>gpt-4o-mini-2024-07-18</b>\n",
    "- <b>gpt-4-0613</b>\n",
    "- <b>gpt-3.5-turbo-0125</b>\n",
    "- <b>gpt-3.5-turbo-1106</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test chat bot with general model or fine-tuning model of choice\n",
    "# *model* should include available OpenAI models for evaluation from link above\n",
    "model = \"MODEL OF CHOICE\"\n",
    "evaluation_prompt = \"INSTRUCTIONS PROMPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute function for testing model against unique samples\n",
    "result = accuracy_testing(df, evaluation_prompt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy result\n",
    "print(f\"Accuracy: {result * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
